# NeoSQLite v1.3.0 Release Notes

## Overview

This release introduces major enhancements to GridFS functionality with PyMongo-style nested access syntax, automatic legacy table migration, and several new aggregation pipeline stages. The GridFS implementation now uses underscore-based table names for better SQLite compatibility while maintaining full backward compatibility with existing databases. New aggregation stages (`$count`, `$sample`, `$unset`, `$facet`) and update operators (`$currentDate`, `$setOnInsert`) bring NeoSQLite closer to full MongoDB API compatibility.

## Highlights

### PyMongo-Style GridFS Access

- **Nested Attribute Access**: Support for `connection.fs.files` and `connection.fs.chunks` syntax enabling drop-in replacement for PyMongo GridFS operations
- **Collection `__getattr__` Support**: Added nested sub-collection access (e.g., `db.fs.files`) for intuitive GridFS file management
- **Direct File Operations**: Update GridFS file metadata using familiar PyMongo syntax

### Automatic GridFS Table Migration

- **Seamless Migration**: Automatic migration from dot-based table names (`fs.files`) to underscore-based names (`fs_files`) for better SQLite compatibility
- **Backward Compatibility**: Existing databases with old table names are automatically migrated on first GridFS access
- **Fallback Protection**: If migration fails, continues using old table names to ensure backward compatibility

### New Aggregation Pipeline Stages

- **`$count`**: MongoDB-compatible count aggregation stage for counting documents
- **`$sample`**: Random sampling aggregation stage with configurable sample size
- **`$unset`**: Field removal aggregation stage for single or multiple fields
- **`$facet`**: Multi-pipeline aggregation stage for running independent pipelines on the same dataset

### New Update Operators

- **`$currentDate`**: Automatically sets field to current date/time during update operations
- **`$setOnInsert`**: Sets field value only during upsert insertions, ignored on updates

## New Features

### Enhanced GridFS Implementation

#### Underscore-Based Table Naming

GridFS collections now use underscore-based table names for better SQLite compatibility:

- **Before**: `fs.files`, `fs.chunks` (dot-based, caused SQLite parsing issues)
- **After**: `fs_files`, `fs_chunks` (underscore-based, clean and compatible)

This change enables PyMongo-like syntax while avoiding SQLite identifier parsing issues with dot notation.

#### Automatic Legacy Table Migration

Existing GridFS tables are automatically migrated during first access:

```python
# Old databases with fs.files/fs.chunks tables are automatically renamed
bucket = db.get_bucket("fs")  # Migration happens transparently
```

Manual migration (if needed):

```sql
ALTER TABLE fs.files RENAME TO fs_files;
ALTER TABLE fs.chunks RENAME TO fs_chunks;
```

#### Metadata Storage Improvements

- **New Databases**: Metadata column created as `JSONB` type (when SQLite supports it) for better performance
- **Existing Databases**: Gradual per-row migration from TEXT to JSONB on metadata updates
- **Transparent Access**: `json()` wrapper ensures both TEXT and JSONB columns read correctly

### Aggregation Pipeline Enhancements

#### `$count` Stage

Count documents matching a filter:

```python
# Basic count
pipeline = [{"$count": "total"}]
result = collection.aggregate(pipeline)
# Output: [{"total": 42}]

# Count with filter
pipeline = [{"$match": {"status": "active"}}, {"$count": "active_count"}]
result = collection.aggregate(pipeline)
```

#### `$sample` Stage

Random sampling of documents:

```python
# Sample 5 random documents
pipeline = [{"$sample": {"size": 5}}]
result = collection.aggregate(pipeline)

# Sample after filtering
pipeline = [{"$match": {"category": "electronics"}}, {"$sample": {"size": 3}}]
result = collection.aggregate(pipeline)
```

#### `$unset` Stage

Remove fields from documents:

```python
# Unset single field
pipeline = [{"$unset": "temporary_field"}]
result = collection.aggregate(pipeline)

# Unset multiple fields
pipeline = [{"$unset": ["field1", "field2", "field3"]}]
result = collection.aggregate(pipeline)

# Unset nested field
pipeline = [{"$unset": "address.zipcode"}]
result = collection.aggregate(pipeline)
```

#### `$facet` Stage

Run multiple independent pipelines on the same dataset:

```python
pipeline = [
    {
        "$facet": {
            "categorized": [{"$match": {"category": "electronics"}}],
            "priced": [{"$match": {"price": {"$gt": 100}}}],
            "total": [{"$count": "count"}]
        }
    }
]
result = collection.aggregate(pipeline)
# Output: [{"categorized": [...], "priced": [...], "total": [{"count": N}]}]
```

### Update Operator Enhancements

#### `$currentDate` Operator

Automatically set field to current date/time:

```python
# Set to current timestamp
collection.update_one(
    {"_id": doc_id},
    {"$currentDate": {"lastModified": True}}
)
```

#### `$setOnInsert` Operator

Set field value only during upsert insertion:

```python
# createdAt only set on insert, not on update
collection.update_one(
    {"_id": doc_id},
    {"$set": {"name": "Updated"}, "$setOnInsert": {"createdAt": "2026-02-24"}},
    upsert=True
)
```

## Query Examples

### GridFS PyMongo-Style Operations

```python
# Upload file with metadata
file_id = bucket.upload_from_stream(
    "document.pdf",
    data,
    metadata={"status": "pending", "category": "temp"}
)

# Update metadata using PyMongo-style nested access
connection.fs.files.update_one(
    {"_id": file_id},
    {"$set": {"metadata": {"status": "processed", "category": "archive"}}}
)

# Find files with filter
cursor = bucket.find({"metadata.status": "processed"})
files = list(cursor)

# Delete file by ID
bucket.delete(file_id)
```

### Aggregation Pipeline Examples

```python
# Count with match filter
pipeline = [{"$match": {"a": {"$gt": 1}}}, {"$count": "filtered_count"}]
result = list(collection.aggregate(pipeline))

# Random sampling
pipeline = [{"$sample": {"size": 10}}]
result = list(collection.aggregate(pipeline))

# Faceted search with post-processing
pipeline = [
    {
        "$facet": {
            "high_value": [{"$match": {"price": {"$gt": 1000}}}],
            "low_value": [{"$match": {"price": {"$lte": 1000}}}]
        }
    },
    {"$project": {"high_count": {"$size": "$high_value"}, "low_count": {"$size": "$low_value"}}}
]
result = list(collection.aggregate(pipeline))
```

## Internal Improvements

### GridFS Architecture Changes

- **Table Naming**: Changed from `bucket.files`/`bucket.chunks` to `bucket_files`/`bucket_chunks` format
- **SQL Query Optimization**: Removed backticks from table names in CREATE INDEX and SELECT statements
- **Migration Logic**: Added `_migrate_legacy_tables_if_needed()` method in `GridFSBucket` class
- **Metadata Handling**: Use `json()` wrapper to ensure JSONB columns convert back to JSON text on read

### Query Engine Enhancements

- **`$facet` Integration**: Handles `$facet` in Python fallback tier, leveraging existing `aggregate_with_constraints` method
- **Sequential Processing**: Facets execute sequentially due to SQLite's single-threaded nature
- **Code Reuse**: 90%+ of existing aggregation code reused without modification

### Type Correction Updates

- **`$currentDate` Support**: Added datetime handling in update operations
- **`$setOnInsert` Logic**: Conditional application based on upsert vs. update operations

## API Changes

### GridFS API Enhancements

The GridFS API now supports PyMongo-style nested attribute access:

- **Attribute Access**: `connection.fs.files` returns a collection-like object for direct file operations
- **Backward Compatibility**: All existing GridFS API methods continue to work unchanged
- **Enhanced Syntax**: More intuitive file management using familiar PyMongo patterns

### Aggregation Pipeline API

New aggregation stages follow MongoDB-compatible syntax:

- **`$count`**: `{"$count": "<output_field_name>"}`
- **`$sample`**: `{"$sample": {"size": <n>}}`
- **`$unset`**: `{"$unset": "<field>"}` or `{"$unset": ["<field1>", "<field2>"]}`
- **`$facet`**: `{"$facet": {"<facet_name>": [<pipeline_stages>]}}`

### Update Operator API

New update operators follow MongoDB-compatible syntax:

- **`$currentDate`**: `{"$currentDate": {"<field>": True}}`
- **`$setOnInsert`**: `{"$setOnInsert": {"<field>": <value>}}`

## Technical Benefits

- **SQLite Compatibility**: Underscore-based table names avoid SQLite identifier parsing issues
- **PyMongo Compatibility**: Drop-in replacement syntax for GridFS operations
- **Enhanced Aggregation**: More powerful data processing capabilities with new pipeline stages
- **Backward Compatibility**: All existing code continues to work unchanged
- **Automatic Migration**: Zero-downtime migration for existing GridFS databases
- **Performance**: JSONB metadata storage for better JSON operation performance

## Migration Notes

### For Existing GridFS Databases

**Automatic Migration**: Existing GridFS tables are automatically renamed during first GridFS initialization:

```python
# No code changes needed - migration happens transparently
bucket = db.get_bucket("fs")  # Old fs.files/fs.chunks tables automatically renamed
```

**Manual Migration** (if needed):

```sql
ALTER TABLE fs.files RENAME TO fs_files;
ALTER TABLE fs.chunks RENAME TO fs_chunks;
```

### For Existing Aggregation Pipelines

All existing aggregation pipelines continue to work unchanged. New stages can be added incrementally:

```python
# Existing pipelines work without modification
pipeline = [{"$match": {"status": "active"}}, {"$group": {"_id": "$category"}}]

# New stages can be added as needed
pipeline = [
    {"$match": {"status": "active"}},
    {"$group": {"_id": "$category"}},
    {"$count": "category_count"}  # New in v1.3.0
]
```

### For Existing Update Operations

All existing update operations continue to work unchanged. New operators can be used alongside existing ones:

```python
# Existing operations unchanged
collection.update_one({"_id": doc_id}, {"$set": {"name": "Updated"}})

# New operators can be combined
collection.update_one(
    {"_id": doc_id},
    {
        "$set": {"name": "Updated"},
        "$currentDate": {"lastModified": True},
        "$setOnInsert": {"createdAt": "2026-02-24"}
    },
    upsert=True
)
```

## Installation

```bash
# Standard installation
pip install neosqlite==1.3.0

# For enhanced JSON/JSONB support
pip install neosqlite[jsonb]==1.3.0

# For memory-constrained processing of large result sets
pip install neosqlite[memory-constrained]==1.3.0

# Install multiple extras
pip install neosqlite[jsonb,memory-constrained]==1.3.0
```

## Documentation

- **Migration Guide**: See `documents/GRIDFS_MIGRATION.md` for detailed migration instructions
- **$facet Implementation**: See `documents/FACET_IMPLEMENTATION.md` for technical details
- **ObjectId Implementation**: See `documents/OBJECTID_IMPLEMENTATION.md` for ObjectId details
- **Missing Features Analysis**: See `documents/GRIDFS_MISSING_FEATURES.md` for future GridFS enhancements

## Testing

This release includes comprehensive test coverage:

- **GridFS Tests**: Enhanced test suite covering PyMongo-style operations and metadata updates
- **Aggregation Tests**: New tests for `$count`, `$sample`, `$unset`, and `$facet` stages
- **Update Operator Tests**: Tests for `$currentDate` and `$setOnInsert` operators
- **Migration Tests**: Tests verifying automatic legacy table migration

This release represents a significant step forward in NeoSQLite's MongoDB compatibility, with enhanced GridFS functionality, powerful aggregation pipeline stages, and improved update operators while maintaining full backward compatibility with existing applications.

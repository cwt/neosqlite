# Nested Array Unwinding with `json_each()`

## Overview

This document provides a detailed explanation of how NeoSQLite handles nested `$unwind` operations by leveraging SQLite's `json_each()` function. This approach significantly improves performance compared to a purely Python-based implementation.

## How It Works: A Comparison

Let's consider the following document and aggregation pipeline to understand the difference between the Python-level and SQLite-level implementations.

**Document:**
```json
{
  "name": "Alice",
  "orders": [
    {
      "orderId": "A001",
      "items": [{"product": "Book"}, {"product": "Pen"}]
    },
    {
      "orderId": "A002",
      "items": [{"product": "Notebook"}]
    }
  ]
}
```

**Pipeline:**
```python
[
  {"$unwind": "$orders"},
  {"$unwind": "$orders.items"}
]
```

---

### 1. The Previous Python-Level Implementation

The pure Python approach is essentially a series of nested loops that process data *after* it has been loaded from the database into the application's memory.

**Step-by-Step Process:**

1.  **Fetch Data:** The first step is to fetch the entire document for "Alice" from SQLite into a Python dictionary.
    ```python
    docs = [{"name": "Alice", "orders": [...]}]
    ```

2.  **Process First `$unwind: "$orders"`:**
    *   Create a new empty list, `unwound_docs_1`.
    *   Loop through each document in `docs` (just Alice's document).
    *   Access the `orders` array, which has two order objects.
    *   Loop through the `orders` array.
        *   **For the first order (`A001`):** Create a *copy* of Alice's document and replace the `orders` field with the first order object. Add it to `unwound_docs_1`.
        *   **For the second order (`A002`):** Create another *copy* of Alice's document and replace the `orders` field with the second order object. Add it to `unwound_docs_1`.
    *   After this stage, `unwound_docs_1` in Python's memory looks like this:
        ```python
        [
          {"name": "Alice", "orders": {"orderId": "A001", "items": [...]}},
          {"name": "Alice", "orders": {"orderId": "A002", "items": [...]}}
        ]
        ```

3.  **Process Second `$unwind: "$orders.items"`:**
    *   Create another new empty list, `unwound_docs_2`.
    *   Loop through each document in `unwound_docs_1`.
        *   **For the first document (`orderId: "A001"`):**
            *   Access the `items` array inside the `orders` object. It has two items ("Book", "Pen").
            *   Loop through this `items` array.
            *   Create a *copy* of the document, add a new field `"orders.items"` with the "Book" object, and add it to `unwound_docs_2`.
            *   Create another *copy*, add `"orders.items"` with the "Pen" object, and add it to `unwound_docs_2`.
        *   **For the second document (`orderId: "A002"`):**
            *   Access its `items` array. It has one item ("Notebook").
            *   Create a *copy*, add `"orders.items"` with the "Notebook" object, and add it to `unwound_docs_2`.
    *   The final `unwound_docs_2` list is the result.

**Drawbacks of this approach:**
*   **High Memory Usage:** All documents and intermediate results are held in Python's memory. For large datasets, this can be very inefficient.
*   **Slow Performance:** Python loops and the continuous creation of document copies are significantly slower than native database operations.
*   **Heavy Data Transfer:** All the raw data has to be transferred from the database to the Python process before any work can begin.

---

### 2. The New SQLite-Level Implementation

This approach offloads the entire unwinding process to the SQLite database engine itself, using its built-in JSON functions. It's a declarative approach: we describe the final result we want, and the database figures out the most efficient way to produce it.

**The Generated SQL Query:**

The key is a single, powerful SQL query that looks like this:

```sql
SELECT
    collection.id,
    json_set(
        json_set(collection.data, '$."orders"', je1.value),
        '$."orders.items"', je2.value
    ) AS data
FROM
    collection,
    json_each(json_extract(collection.data, '$.orders')) AS je1,
    json_each(json_extract(je1.value, '$.items')) AS je2
WHERE
    collection.id = 1; -- (Assuming we're looking at Alice's document)
```

**How it Works:**

1.  **`FROM collection, json_each(...), json_each(...)`**: This is the most critical part. The commas in the `FROM` clause create a `CROSS JOIN` between the main table and the virtual tables generated by `json_each`.

2.  **First `json_each` (`je1`):**
    *   `json_each(json_extract(collection.data, '$.orders'))` operates on the `orders` array of the original document.
    *   It effectively creates a temporary, in-memory table with one row for each element in the `orders` array. For Alice's document, this produces two rows. The `value` column of this virtual table (`je1.value`) holds the complete order object (`{"orderId": "A001", ...}`).

3.  **Second `json_each` (`je2`):**
    *   This is the "nested" part. `json_each(json_extract(je1.value, '$.items'))` operates on the *result* of the first `json_each`.
    *   For each row produced by `je1`, it takes the `order` object and expands its `items` array.
    *   For the "A001" order, it produces two rows (one for "Book", one for "Pen").
    *   For the "A002" order, it produces one row (for "Notebook").

4.  **The `CROSS JOIN` Effect:** The database engine combines these expansions. It joins each original document with each of its unwound orders, and then joins that result with each of that order's unwound items. This produces the final 3 rows at the database level, directly achieving the Cartesian product.

5.  **`SELECT ... json_set(...)`:** For each of the 3 final rows generated by the `FROM` clause, the `SELECT` statement constructs the final JSON document.
    *   The inner `json_set` takes the original document and replaces the entire `orders` array with the single unwound `order` object from `je1`.
    *   The outer `json_set` takes that result and adds the new field `orders.items` with the value of the unwound `item` from `je2`.

**Advantages of this approach:**
*   **Low Memory Usage:** All the heavy lifting and expansion happens within the highly optimized SQLite engine. Python's memory is only used to hold the *final* results, not the intermediate ones.
*   **High Performance:** The entire operation is performed in compiled C code within the database, which is orders of magnitude faster than Python loops.
*   **Minimal Data Transfer:** Only the final 3 processed documents are transferred from the database to the Python process.

### Summary Comparison

| Aspect | Python-Level Implementation | SQLite-Level Implementation |
| :--- | :--- | :--- |
| **Processing Location** | In the Python application's memory | Inside the SQLite database engine |
| **Core Mechanism** | Nested loops and object copying | `CROSS JOIN` of virtual tables from `json_each` |
| **Memory Usage** | **Very High.** Stores original docs, intermediate lists, and final results. | **Very Low.** The database manages memory. Python only stores the final result set. |
| **Performance** | **Slow.** Limited by Python's interpretation speed and object overhead. | **Extremely Fast.** Operations run in SQLite's native, compiled C code. |
| **Data Transfer** | **High.** The full, unprocessed document is sent to Python first. | **Low.** A single query is sent; only the final, processed rows are returned. |
